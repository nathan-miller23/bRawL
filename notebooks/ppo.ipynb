{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ray\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakout_env = gym.make('Breakout-v0')\n",
    "\n",
    "ob_space = breakout_env.observation_space\n",
    "ac_space = breakout_env.action_space\n",
    "\n",
    "initial_state = breakout_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # TODO: Set some instance variables here\n",
    "        # Note: the super constructor will set the instance variables `action_space` and `observation_space` for you\n",
    "        pass\n",
    "    \n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        **kwargs):\n",
    "        # Randomly sample an action\n",
    "        pass\n",
    "    \n",
    "    def learn_on_batch(self, samples):\n",
    "        \"\"\"No learning.\"\"\"\n",
    "        return {}\n",
    "    \n",
    "    def compute_log_likelihoods(self,\n",
    "                                actions,\n",
    "                                obs_batch,\n",
    "                                state_batches=None,\n",
    "                                prev_action_batch=None,\n",
    "                                prev_reward_batch=None):\n",
    "        # Return logits from a uniform distribution\n",
    "        pass\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        No-op to keep rllib from breaking, won't be necessary in future rllib releases\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"\n",
    "        No-op to keep rllib from breaking\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "class BreakoutRllib(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._env = gym.make('Breakout-v0')\n",
    "        self.action_space = self._env.action_space\n",
    "        self.observation_space = self._env.observation_space\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self._env.step(action)\n",
    "        return { \"random\" : obs }, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return self._env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-04 22:14:15,766\tINFO resource_spec.py:231 -- Starting Ray with 0.88 GiB memory available for workers and up to 0.45 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-10-04 22:14:16,845\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n",
      "2020-10-04 22:14:16,850\tWARNING services.py:1567 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n",
      "2020-10-04 22:14:16,884\tWARNING services.py:1567 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'breakout_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-72159ba1d0db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m register_env(\"breakout\",\n\u001b[1;32m      8\u001b[0m              lambda _: BreakoutRllib())\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mobs_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbreakout_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mact_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbreakout_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'breakout_env' is not defined"
     ]
    }
   ],
   "source": [
    "# Some ray setup\n",
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "from ray.tune.registry import register_env\n",
    "ray.init()\n",
    "\n",
    "# Register the gym environment (Note that this is different than the gym registry!)\n",
    "register_env(\"breakout\",\n",
    "             lambda _: BreakoutRllib())\n",
    "obs_space = breakout_env.observation_space\n",
    "act_space = breakout_env.action_space\n",
    "\n",
    "config = config={\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "                \"random\": (RandomPolicy, obs_space, act_space, {}),\n",
    "            },\n",
    "            \"policy_mapping_fn\": (\n",
    "                lambda agent_id: \"random\"),\n",
    "        },\n",
    "        \"num_workers\" : 1,\n",
    "        \"framework\": \"torch\"\n",
    "}\n",
    "\n",
    "trainer = PPOTrainer(env=\"breakout\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
