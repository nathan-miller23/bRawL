{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import Dict, List, Type, Union\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.a3c.a3c_torch_policy import apply_grad_clipping\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import postprocess_ppo_gae, \\\n",
    "    setup_config\n",
    "from ray.rllib.evaluation.postprocessing import Postprocessing\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchDistributionWrapper\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.policy.torch_policy import EntropyCoeffSchedule, \\\n",
    "    LearningRateSchedule\n",
    "from ray.rllib.policy.torch_policy_template import build_torch_policy\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.torch_ops import convert_to_torch_tensor, \\\n",
    "    explained_variance, sequence_mask\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def ppo_surrogate_loss(policy, model, dist_class, train_batch):\n",
    "    \"\"\"Constructs the loss for Proximal Policy Objective.\n",
    "    Args:\n",
    "        policy (Policy): The Policy to calculate the loss for.\n",
    "        model (ModelV2): The Model to calculate the loss for.\n",
    "        dist_class (Type[ActionDistribution]: The action distr. class.\n",
    "        train_batch (SampleBatch): The training data.\n",
    "    Returns:\n",
    "        Union[TensorType, List[TensorType]]: A single loss tensor or a list\n",
    "            of loss tensors.\n",
    "    \"\"\"\n",
    "    logits, state = model.from_batch(train_batch, is_training=True)\n",
    "    curr_action_dist = dist_class(logits, model)\n",
    "\n",
    "    # RNN case: Mask away 0-padded chunks at end of time axis.\n",
    "    if state:\n",
    "        max_seq_len = torch.max(train_batch[\"seq_lens\"])\n",
    "        mask = sequence_mask(\n",
    "            train_batch[\"seq_lens\"],\n",
    "            max_seq_len,\n",
    "            time_major=model.is_time_major())\n",
    "        mask = torch.reshape(mask, [-1])\n",
    "        num_valid = torch.sum(mask)\n",
    "\n",
    "        def reduce_mean_valid(t):\n",
    "            return torch.sum(t[mask]) / num_valid\n",
    "\n",
    "    # non-RNN case: No masking.\n",
    "    else:\n",
    "        mask = None\n",
    "        reduce_mean_valid = torch.mean\n",
    "\n",
    "    prev_action_dist = dist_class(train_batch[SampleBatch.ACTION_DIST_INPUTS],\n",
    "                                  model)\n",
    "\n",
    "    logp_ratio = torch.exp(\n",
    "        curr_action_dist.logp(train_batch[SampleBatch.ACTIONS]) -\n",
    "        train_batch[SampleBatch.ACTION_LOGP])\n",
    "    action_kl = prev_action_dist.kl(curr_action_dist)\n",
    "    mean_kl = reduce_mean_valid(action_kl)\n",
    "\n",
    "    curr_entropy = curr_action_dist.entropy()\n",
    "    mean_entropy = reduce_mean_valid(curr_entropy)\n",
    "\n",
    "    surrogate_loss = torch.min(\n",
    "        train_batch[Postprocessing.ADVANTAGES] * logp_ratio,\n",
    "        train_batch[Postprocessing.ADVANTAGES] * torch.clamp(\n",
    "            logp_ratio, 1 - policy.config[\"clip_param\"],\n",
    "            1 + policy.config[\"clip_param\"]))\n",
    "    mean_policy_loss = reduce_mean_valid(-surrogate_loss)\n",
    "\n",
    "    if policy.config[\"use_gae\"]:\n",
    "        prev_value_fn_out = train_batch[SampleBatch.VF_PREDS]\n",
    "        value_fn_out = model.value_function()\n",
    "        vf_loss1 = torch.pow(\n",
    "            value_fn_out - train_batch[Postprocessing.VALUE_TARGETS], 2.0)\n",
    "        vf_clipped = prev_value_fn_out + torch.clamp(\n",
    "            value_fn_out - prev_value_fn_out, -policy.config[\"vf_clip_param\"],\n",
    "            policy.config[\"vf_clip_param\"])\n",
    "        vf_loss2 = torch.pow(\n",
    "            vf_clipped - train_batch[Postprocessing.VALUE_TARGETS], 2.0)\n",
    "        vf_loss = torch.max(vf_loss1, vf_loss2)\n",
    "        mean_vf_loss = reduce_mean_valid(vf_loss)\n",
    "        total_loss = reduce_mean_valid(\n",
    "            -surrogate_loss + policy.kl_coeff * action_kl +\n",
    "            policy.config[\"vf_loss_coeff\"] * vf_loss -\n",
    "            policy.entropy_coeff * curr_entropy)\n",
    "    else:\n",
    "        mean_vf_loss = 0.0\n",
    "        total_loss = reduce_mean_valid(-surrogate_loss +\n",
    "                                       policy.kl_coeff * action_kl -\n",
    "                                       policy.entropy_coeff * curr_entropy)\n",
    "\n",
    "    # Store stats in policy for stats_fn.\n",
    "    policy._total_loss = total_loss\n",
    "    policy._mean_policy_loss = mean_policy_loss\n",
    "    policy._mean_vf_loss = mean_vf_loss\n",
    "    policy._mean_entropy = mean_entropy\n",
    "    policy._mean_kl = mean_kl\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def kl_and_loss_stats(policy, train_batch):\n",
    "    \"\"\"Stats function for PPO. Returns a dict with important KL and loss stats.\n",
    "    Args:\n",
    "        policy (Policy): The Policy to generate stats for.\n",
    "        train_batch (SampleBatch): The SampleBatch (already) used for training.\n",
    "    Returns:\n",
    "        Dict[str, TensorType]: The stats dict.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"cur_kl_coeff\": policy.kl_coeff,\n",
    "        \"cur_lr\": policy.cur_lr,\n",
    "        \"total_loss\": policy._total_loss,\n",
    "        \"policy_loss\": policy._mean_policy_loss,\n",
    "        \"vf_loss\": policy._mean_vf_loss,\n",
    "        \"vf_explained_var\": explained_variance(\n",
    "            train_batch[Postprocessing.VALUE_TARGETS],\n",
    "            policy.model.value_function()),\n",
    "        \"kl\": policy._mean_kl,\n",
    "        \"entropy\": policy._mean_entropy,\n",
    "        \"entropy_coeff\": policy.entropy_coeff,\n",
    "    }\n",
    "\n",
    "\n",
    "def vf_preds_fetches(policy, input_dict,state_batches, model, action_dist):\n",
    "    \"\"\"Defines extra fetches per action computation.\n",
    "    Args:\n",
    "        policy (Policy): The Policy to perform the extra action fetch on.\n",
    "        input_dict (Dict[str, TensorType]): The input dict used for the action\n",
    "            computing forward pass.\n",
    "        state_batches (List[TensorType]): List of state tensors (empty for\n",
    "            non-RNNs).\n",
    "        model (ModelV2): The Model object of the Policy.\n",
    "        action_dist (TorchDistributionWrapper): The instantiated distribution\n",
    "            object, resulting from the model's outputs and the given\n",
    "            distribution class.\n",
    "    Returns:\n",
    "        Dict[str, TensorType]: Dict with extra tf fetches to perform per\n",
    "            action computation.\n",
    "    \"\"\"\n",
    "    # Return value function outputs. VF estimates will hence be added to the\n",
    "    # SampleBatches produced by the sampler(s) to generate the train batches\n",
    "    # going into the loss function.\n",
    "    return {\n",
    "        SampleBatch.VF_PREDS: policy.model.value_function(),\n",
    "    }\n",
    "\n",
    "\n",
    "class KLCoeffMixin:\n",
    "    \"\"\"Assigns the `update_kl()` method to the PPOPolicy.\n",
    "    This is used in PPO's execution plan (see ppo.py) for updating the KL\n",
    "    coefficient after each learning step based on `config.kl_target` and\n",
    "    the measured KL value (from the train_batch).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        # The current KL value (as python float).\n",
    "        self.kl_coeff = config[\"kl_coeff\"]\n",
    "        # Constant target value.\n",
    "        self.kl_target = config[\"kl_target\"]\n",
    "\n",
    "    def update_kl(self, sampled_kl):\n",
    "        # Update the current KL value based on the recently measured value.\n",
    "        if sampled_kl > 2.0 * self.kl_target:\n",
    "            self.kl_coeff *= 1.5\n",
    "        elif sampled_kl < 0.5 * self.kl_target:\n",
    "            self.kl_coeff *= 0.5\n",
    "        # Return the current KL value.\n",
    "        return self.kl_coeff\n",
    "\n",
    "\n",
    "class ValueNetworkMixin:\n",
    "    \"\"\"Assigns the `_value()` method to the PPOPolicy.\n",
    "    This way, Policy can call `_value()` to get the current VF estimate on a\n",
    "    single(!) observation (as done in `postprocess_trajectory_fn`).\n",
    "    Note: When doing this, an actual forward pass is being performed.\n",
    "    This is different from only calling `model.value_function()`, where\n",
    "    the result of the most recent forward pass is being used to return an\n",
    "    already calculated tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        # When doing GAE, we need the value function estimate on the\n",
    "        # observation.\n",
    "        if config[\"use_gae\"]:\n",
    "\n",
    "            def value(ob, prev_action, prev_reward, *state):\n",
    "                model_out, _ = self.model({\n",
    "                    SampleBatch.CUR_OBS: convert_to_torch_tensor(\n",
    "                        np.asarray([ob]), self.device),\n",
    "                    SampleBatch.PREV_ACTIONS: convert_to_torch_tensor(\n",
    "                        np.asarray([prev_action]), self.device),\n",
    "                    SampleBatch.PREV_REWARDS: convert_to_torch_tensor(\n",
    "                        np.asarray([prev_reward]), self.device),\n",
    "                    \"is_training\": False,\n",
    "                }, [\n",
    "                    convert_to_torch_tensor(np.asarray([s]), self.device)\n",
    "                    for s in state\n",
    "                ], convert_to_torch_tensor(np.asarray([1]), self.device))\n",
    "                # [0] = remove the batch dim.\n",
    "                return self.model.value_function()[0]\n",
    "\n",
    "        # When not doing GAE, we do not require the value function's output.\n",
    "        else:\n",
    "\n",
    "            def value(ob, prev_action, prev_reward, *state):\n",
    "                return 0.0\n",
    "\n",
    "        self._value = value\n",
    "\n",
    "\n",
    "def setup_mixins(policy, obs_space, action_space, config):\n",
    "    \"\"\"Call all mixin classes' constructors before PPOPolicy initialization.\n",
    "    Args:\n",
    "        policy (Policy): The Policy object.\n",
    "        obs_space (gym.spaces.Space): The Policy's observation space.\n",
    "        action_space (gym.spaces.Space): The Policy's action space.\n",
    "        config (TrainerConfigDict): The Policy's config.\n",
    "    \"\"\"\n",
    "    ValueNetworkMixin.__init__(policy, obs_space, action_space, config)\n",
    "    KLCoeffMixin.__init__(policy, config)\n",
    "    EntropyCoeffSchedule.__init__(policy, config[\"entropy_coeff\"],\n",
    "                                  config[\"entropy_coeff_schedule\"])\n",
    "    LearningRateSchedule.__init__(policy, config[\"lr\"], config[\"lr_schedule\"])\n",
    "\n",
    "\n",
    "# Build a child class of `TorchPolicy`, given the custom functions defined\n",
    "# above.\n",
    "PPOTorchPolicy = build_torch_policy(\n",
    "    name=\"PPOTorchPolicy\",\n",
    "    get_default_config=lambda: ray.rllib.agents.ppo.ppo.DEFAULT_CONFIG,\n",
    "    loss_fn=ppo_surrogate_loss,\n",
    "    stats_fn=kl_and_loss_stats,\n",
    "    extra_action_out_fn=vf_preds_fetches,\n",
    "    postprocess_fn=postprocess_ppo_gae,\n",
    "    extra_grad_process_fn=apply_grad_clipping,\n",
    "    before_init=setup_config,\n",
    "    after_init=setup_mixins,\n",
    "    mixins=[\n",
    "        LearningRateSchedule, EntropyCoeffSchedule, KLCoeffMixin,\n",
    "        ValueNetworkMixin\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "class BreakoutRllib(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._env = gym.make('Breakout-v0')\n",
    "        self.action_space = self._env.action_space\n",
    "        self.observation_space = self._env.observation_space\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self._env.step(action)\n",
    "        return { \"random\" : obs }, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return self._env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakout_env = gym.make('Breakout-v0')\n",
    "\n",
    "ob_space = breakout_env.observation_space\n",
    "ac_space = breakout_env.action_space\n",
    "\n",
    "initial_state = breakout_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ad8bbc6c89c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m\"framework\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m }\n\u001b[0;32m--> 283\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Breakout-v0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mlogger_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_logger_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, logger_creator)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mget_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;31m# Evaluation setup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;31m# Creating all workers (excluding evaluation workers).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             self.workers = self._make_workers(\n\u001b[0;32m--> 102\u001b[0;31m                 env_creator, self._policy, config, self.config[\"num_workers\"])\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_plan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_plan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_exec_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_plan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_make_workers\u001b[0;34m(self, env_creator, policy, config, num_workers)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             logdir=self.logdir)\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mDeveloperAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, policy, trainer_config, num_workers, logdir, _setup)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Always create a local worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             self._local_worker = self._make_worker(\n\u001b[0;32m---> 67\u001b[0;31m                 RolloutWorker, env_creator, policy, 0, self._local_config)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Create a number of remote workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m_make_worker\u001b[0;34m(self, cls, env_creator, policy, worker_index, config)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mfake_sampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fake_sampler\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             extra_python_environs=extra_python_environs)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, policy, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, observation_filter, clip_rewards, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, monitor_path, log_dir, log_level, callbacks, input_creator, input_evaluation, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler)\u001b[0m\n\u001b[1;32m    388\u001b[0m                 tf1 and tf1.executing_eagerly()):\n\u001b[1;32m    389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not import tensorflow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtf_session_creator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import tensorflow"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import Optional, Type\n",
    "\n",
    "from ray.rllib.agents import with_common_config\n",
    "from ray.rllib.agents.trainer_template import build_trainer\n",
    "from ray.rllib.evaluation.worker_set import WorkerSet\n",
    "from ray.rllib.execution.rollout_ops import ParallelRollouts, ConcatBatches, \\\n",
    "    StandardizeFields, SelectExperiences\n",
    "from ray.rllib.execution.train_ops import TrainOneStep, TrainTFMultiGPU\n",
    "from ray.rllib.execution.metric_ops import StandardMetricsReporting\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.util.iter import LocalIterator\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# yapf: disable\n",
    "# __sphinx_doc_begin__\n",
    "\n",
    "# Adds the following updates to the (base) `Trainer` config in\n",
    "# rllib/agents/trainer.py (`COMMON_CONFIG` dict).\n",
    "DEFAULT_CONFIG = with_common_config({\n",
    "    # Should use a critic as a baseline (otherwise don't use value baseline;\n",
    "    # required for using GAE).\n",
    "    \"use_critic\": True,\n",
    "    # If true, use the Generalized Advantage Estimator (GAE)\n",
    "    # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.\n",
    "    \"use_gae\": True,\n",
    "    # The GAE(lambda) parameter.\n",
    "    \"lambda\": 1.0,\n",
    "    # Initial coefficient for KL divergence.\n",
    "    \"kl_coeff\": 0.2,\n",
    "    # Size of batches collected from each worker.\n",
    "    \"rollout_fragment_length\": 200,\n",
    "    # Number of timesteps collected for each SGD round. This defines the size\n",
    "    # of each SGD epoch.\n",
    "    \"train_batch_size\": 4000,\n",
    "    # Total SGD batch size across all devices for SGD. This defines the\n",
    "    # minibatch size within each epoch.\n",
    "    \"sgd_minibatch_size\": 128,\n",
    "    # Whether to shuffle sequences in the batch when training (recommended).\n",
    "    \"shuffle_sequences\": True,\n",
    "    # Number of SGD iterations in each outer loop (i.e., number of epochs to\n",
    "    # execute per train batch).\n",
    "    \"num_sgd_iter\": 30,\n",
    "    # Stepsize of SGD.\n",
    "    \"lr\": 5e-5,\n",
    "    # Learning rate schedule.\n",
    "    \"lr_schedule\": None,\n",
    "    # Share layers for value function. If you set this to True, it's important\n",
    "    # to tune vf_loss_coeff.\n",
    "    \"vf_share_layers\": False,\n",
    "    # Coefficient of the value function loss. IMPORTANT: you must tune this if\n",
    "    # you set vf_share_layers: True.\n",
    "    \"vf_loss_coeff\": 1.0,\n",
    "    # Coefficient of the entropy regularizer.\n",
    "    \"entropy_coeff\": 0.0,\n",
    "    # Decay schedule for the entropy regularizer.\n",
    "    \"entropy_coeff_schedule\": None,\n",
    "    # PPO clip parameter.\n",
    "    \"clip_param\": 0.3,\n",
    "    # Clip param for the value function. Note that this is sensitive to the\n",
    "    # scale of the rewards. If your expected V is large, increase this.\n",
    "    \"vf_clip_param\": 10.0,\n",
    "    # If specified, clip the global norm of gradients by this amount.\n",
    "    \"grad_clip\": None,\n",
    "    # Target value for KL divergence.\n",
    "    \"kl_target\": 0.01,\n",
    "    # Whether to rollout \"complete_episodes\" or \"truncate_episodes\".\n",
    "    \"batch_mode\": \"truncate_episodes\",\n",
    "    # Which observation filter to apply to the observation.\n",
    "    \"observation_filter\": \"NoFilter\",\n",
    "    # Uses the sync samples optimizer instead of the multi-gpu one. This is\n",
    "    # usually slower, but you might want to try it if you run into issues with\n",
    "    # the default optimizer.\n",
    "    \"simple_optimizer\": False,\n",
    "    # Whether to fake GPUs (using CPUs).\n",
    "    # Set this to True for debugging on non-GPU machines (set `num_gpus` > 0).\n",
    "    \"_fake_gpus\": False,\n",
    "    # Switch on Trajectory View API for PPO by default.\n",
    "    # NOTE: Only supported for PyTorch so far.\n",
    "    \"_use_trajectory_view_api\": True,\n",
    "})\n",
    "\n",
    "# __sphinx_doc_end__\n",
    "# yapf: enable\n",
    "\n",
    "\n",
    "def validate_config(config):\n",
    "    \"\"\"Validates the Trainer's config dict.\n",
    "    Args:\n",
    "        config (TrainerConfigDict): The Trainer's config to check.\n",
    "    Raises:\n",
    "        ValueError: In case something is wrong with the config.\n",
    "    \"\"\"\n",
    "    if isinstance(config[\"entropy_coeff\"], int):\n",
    "        config[\"entropy_coeff\"] = float(config[\"entropy_coeff\"])\n",
    "\n",
    "    if config[\"entropy_coeff\"] < 0.0:\n",
    "        raise DeprecationWarning(\"entropy_coeff must be >= 0.0\")\n",
    "\n",
    "    # SGD minibatch size must be smaller than train_batch_size (b/c\n",
    "    # we subsample a batch of `sgd_minibatch_size` from the train-batch for\n",
    "    # each `sgd_num_iter`).\n",
    "    if config[\"sgd_minibatch_size\"] > config[\"train_batch_size\"]:\n",
    "        raise ValueError(\"`sgd_minibatch_size` ({}) must be <= \"\n",
    "                         \"`train_batch_size` ({}).\".format(\n",
    "                             config[\"sgd_minibatch_size\"],\n",
    "                             config[\"train_batch_size\"]))\n",
    "\n",
    "    # Episodes may only be truncated (and passed into PPO's\n",
    "    # `postprocessing_fn`), iff generalized advantage estimation is used\n",
    "    # (value function estimate at end of truncated episode to estimate\n",
    "    # remaining value).\n",
    "    if config[\"batch_mode\"] == \"truncate_episodes\" and not config[\"use_gae\"]:\n",
    "        raise ValueError(\n",
    "            \"Episode truncation is not supported without a value \"\n",
    "            \"function (to estimate the return at the end of the truncated \"\n",
    "            \"trajectory). Consider setting batch_mode=complete_episodes.\")\n",
    "\n",
    "    # Multi-gpu not supported for PyTorch and tf-eager.\n",
    "    if config[\"framework\"] in [\"tf2\", \"tfe\", \"torch\"]:\n",
    "        config[\"simple_optimizer\"] = True\n",
    "    # Performance warning, if \"simple\" optimizer used with (static-graph) tf.\n",
    "    elif config[\"simple_optimizer\"]:\n",
    "        logger.warning(\n",
    "            \"Using the simple minibatch optimizer. This will significantly \"\n",
    "            \"reduce performance, consider simple_optimizer=False.\")\n",
    "    # Multi-agent mode and multi-GPU optimizer.\n",
    "    elif config[\"multiagent\"][\"policies\"] and not config[\"simple_optimizer\"]:\n",
    "        logger.info(\n",
    "            \"In multi-agent mode, policies will be optimized sequentially \"\n",
    "            \"by the multi-GPU optimizer. Consider setting \"\n",
    "            \"simple_optimizer=True if this doesn't work for you.\")\n",
    "\n",
    "\n",
    "def get_policy_class(config):\n",
    "    \"\"\"Policy class picker function. Class is chosen based on DL-framework.\n",
    "    Args:\n",
    "        config (TrainerConfigDict): The trainer's configuration dict.\n",
    "    Returns:\n",
    "        Optional[Type[Policy]]: The Policy class to use with PPOTrainer.\n",
    "            If None, use `default_policy` provided in build_trainer().\n",
    "    \"\"\"\n",
    "    if config[\"framework\"] == \"torch\":\n",
    "        from ray.rllib.agents.ppo.ppo_torch_policy import PPOTorchPolicy\n",
    "        return PPOTorchPolicy\n",
    "\n",
    "\n",
    "class UpdateKL:\n",
    "    \"\"\"Callback to update the KL based on optimization info.\n",
    "    This is used inside the execution_plan function. The Policy must define\n",
    "    a `update_kl` method for this to work. This is achieved for PPO via a\n",
    "    Policy mixin class (which adds the `update_kl` method),\n",
    "    defined in ppo_[tf|torch]_policy.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, workers):\n",
    "        self.workers = workers\n",
    "\n",
    "    def __call__(self, fetches):\n",
    "        def update(pi, pi_id):\n",
    "            assert \"kl\" not in fetches, (\n",
    "                \"kl should be nested under policy id key\", fetches)\n",
    "            if pi_id in fetches:\n",
    "                assert \"kl\" in fetches[pi_id], (fetches, pi_id)\n",
    "                # Make the actual `Policy.update_kl()` call.\n",
    "                pi.update_kl(fetches[pi_id][\"kl\"])\n",
    "            else:\n",
    "                logger.warning(\"No data for {}, not updating kl\".format(pi_id))\n",
    "\n",
    "        # Update KL on all trainable policies within the local (trainer)\n",
    "        # Worker.\n",
    "        self.workers.local_worker().foreach_trainable_policy(update)\n",
    "\n",
    "\n",
    "def warn_about_bad_reward_scales(config, result):\n",
    "    if result[\"policy_reward_mean\"]:\n",
    "        return result  # Punt on handling multiagent case.\n",
    "\n",
    "    # Warn about excessively high VF loss.\n",
    "    learner_stats = result[\"info\"][\"learner\"]\n",
    "    if \"default_policy\" in learner_stats:\n",
    "        scaled_vf_loss = (config[\"vf_loss_coeff\"] *\n",
    "                          learner_stats[\"default_policy\"][\"vf_loss\"])\n",
    "        policy_loss = learner_stats[\"default_policy\"][\"policy_loss\"]\n",
    "        if config[\"vf_share_layers\"] and scaled_vf_loss > 100:\n",
    "            logger.warning(\n",
    "                \"The magnitude of your value function loss is extremely large \"\n",
    "                \"({}) compared to the policy loss ({}). This can prevent the \"\n",
    "                \"policy from learning. Consider scaling down the VF loss by \"\n",
    "                \"reducing vf_loss_coeff, or disabling vf_share_layers.\".format(\n",
    "                    scaled_vf_loss, policy_loss))\n",
    "\n",
    "    # Warn about bad clipping configs\n",
    "    if config[\"vf_clip_param\"] <= 0:\n",
    "        rew_scale = float(\"inf\")\n",
    "    else:\n",
    "        rew_scale = round(\n",
    "            abs(result[\"episode_reward_mean\"]) / config[\"vf_clip_param\"], 0)\n",
    "    if rew_scale > 200:\n",
    "        logger.warning(\n",
    "            \"The magnitude of your environment rewards are more than \"\n",
    "            \"{}x the scale of `vf_clip_param`. \".format(rew_scale) +\n",
    "            \"This means that it will take more than \"\n",
    "            \"{} iterations for your value \".format(rew_scale) +\n",
    "            \"function to converge. If this is not intended, consider \"\n",
    "            \"increasing `vf_clip_param`.\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def execution_plan(workers, config):\n",
    "    \"\"\"Execution plan of the PPO algorithm. Defines the distributed dataflow.\n",
    "    Args:\n",
    "        workers (WorkerSet): The WorkerSet for training the Polic(y/ies)\n",
    "            of the Trainer.\n",
    "        config (TrainerConfigDict): The trainer's configuration dict.\n",
    "    Returns:\n",
    "        LocalIterator[dict]: The Policy class to use with PPOTrainer.\n",
    "            If None, use `default_policy` provided in build_trainer().\n",
    "    \"\"\"\n",
    "    rollouts = ParallelRollouts(workers, mode=\"bulk_sync\")\n",
    "\n",
    "    # Collect batches for the trainable policies.\n",
    "    rollouts = rollouts.for_each(\n",
    "        SelectExperiences(workers.trainable_policies()))\n",
    "    # Concatenate the SampleBatches into one.\n",
    "    rollouts = rollouts.combine(\n",
    "        ConcatBatches(min_batch_size=config[\"train_batch_size\"]))\n",
    "    # Standardize advantages.\n",
    "    rollouts = rollouts.for_each(StandardizeFields([\"advantages\"]))\n",
    "\n",
    "    # Perform one training step on the combined + standardized batch.\n",
    "    if config[\"simple_optimizer\"]:\n",
    "        train_op = rollouts.for_each(\n",
    "            TrainOneStep(\n",
    "                workers,\n",
    "                num_sgd_iter=config[\"num_sgd_iter\"],\n",
    "                sgd_minibatch_size=config[\"sgd_minibatch_size\"]))\n",
    "    else:\n",
    "        train_op = rollouts.for_each(\n",
    "            TrainTFMultiGPU(\n",
    "                workers,\n",
    "                sgd_minibatch_size=config[\"sgd_minibatch_size\"],\n",
    "                num_sgd_iter=config[\"num_sgd_iter\"],\n",
    "                num_gpus=config[\"num_gpus\"],\n",
    "                rollout_fragment_length=config[\"rollout_fragment_length\"],\n",
    "                num_envs_per_worker=config[\"num_envs_per_worker\"],\n",
    "                train_batch_size=config[\"train_batch_size\"],\n",
    "                shuffle_sequences=config[\"shuffle_sequences\"],\n",
    "                _fake_gpus=config[\"_fake_gpus\"],\n",
    "                framework=config.get(\"framework\")))\n",
    "\n",
    "    # Update KL after each round of training.\n",
    "    train_op = train_op.for_each(lambda t: t[1]).for_each(UpdateKL(workers))\n",
    "\n",
    "    # Warn about bad reward scales and return training metrics.\n",
    "    return StandardMetricsReporting(train_op, workers, config) \\\n",
    "        .for_each(lambda result: warn_about_bad_reward_scales(config, result))\n",
    "\n",
    "\n",
    "# Build a child class of `Trainer`, which uses the framework specific Policy\n",
    "# determined in `get_policy_class()` above.\n",
    "PPOTrainer = build_trainer(\n",
    "    name=\"PPO\",\n",
    "    default_config=DEFAULT_CONFIG,\n",
    "    validate_config=validate_config,\n",
    "    default_policy=PPOTFPolicy,\n",
    "    get_policy_class=get_policy_class,\n",
    "    execution_plan=execution_plan,\n",
    ")\n",
    "config={\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"ppo\": (PPOTFPolicy, ob_space, ac_space, {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": (\n",
    "            lambda agent_id: \"random\"),\n",
    "    },\n",
    "    \"num_workers\" : 1,\n",
    "    \"framework\": \"torch\"\n",
    "}\n",
    "trainer = PPOTrainer(env=\"Breakout-v0\", config=config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
