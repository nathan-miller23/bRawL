{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import ray\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents import ppo\n",
    "from ray import tune\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "#Edit default config to do hyperparameter search\n",
    "config['framework'] = 'torch'\n",
    "config['lr'] = 0.001\n",
    "config[\"num_gpus\"] = 2\n",
    "config[\"env\"] = \"BreakoutNoFrameskip-v4\"\n",
    "config[\"preprocessor_pref\"] = \"deepmind\"\n",
    "config[\"num_workers\"]=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ConvNet(TorchModelV2, nn.Module):\n",
    "    \"\"\"Generic fully connected network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        in_channels = obs_space.shape[-1]\n",
    "        self._conv_layers = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, 8, kernel_size=[7,7], padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2, padding=1),\n",
    "            torch.nn.Conv2d(8, 16, kernel_size=[5,5], padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2, padding=1),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=[3,3], padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2, padding=1),\n",
    "            torch.nn.Conv2d(32, num_outputs, kernel_size=[12,12])\n",
    "        )\n",
    "        self._features = None\n",
    "        self._num_outputs = num_outputs\n",
    "\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict[\"obs\"].float().permute(0,3,1,2) #reshape input\n",
    "        self._features = self._conv_layers(obs).view(-1, self._num_outputs)\n",
    "        return self._features, state\n",
    "    \n",
    "    def value_function(self):\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOWrapper:\n",
    "    def __init__(self, env, config, local_dir):\n",
    "        self.agent = None\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "    def train(self, num_steps):\n",
    "        self.agent = ppo.PPOTrainer(config=self.config, env=self.env)\n",
    "        for i in range(num_steps):\n",
    "            result = self.agent.train()\n",
    "            print(\"Iteration: {}\".format(i))\n",
    "            print(\"Reward: {}\", result['episode_reward_mean'])\n",
    "        if i == num_steps - 1:\n",
    "            checkpoint = self.agent.save()\n",
    "            print('checkpoint saved at', checkpoint)\n",
    "        return checkpoint\n",
    "    def load(self, path):\n",
    "        self.agent = ppo.PPOTrainer(config=self.config, env=self.env)\n",
    "        self.agent.restore(path)\n",
    "    def test(self, num_episodes):\n",
    "        env = self.agent.workers.local_worker().env\n",
    "        for episode in range(num_episodes):\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            obs = env.reset()\n",
    "            while not done:\n",
    "                action = self.agent.compute_action(obs)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                plt.imshow(env.render(mode='rgb_array'))\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "                episode_reward += reward\n",
    "            print(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "ppo_agent = PPOWrapper(\"BreakoutNoFrameskip-v4\", config)\n",
    "trainingSteps = 500\n",
    "checkpoint_path = ppo_agent.train(trainingSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent.load(checkpoint_path)\n",
    "ppo_agent.test(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
