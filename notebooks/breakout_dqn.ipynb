{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ray\n",
    "import gym\n",
    "from IPython import display\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-11 16:42:58,521\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionNetwork_as_DQNTorchModel(\n",
      "  (_convs): Sequential(\n",
      "    (0): SlimConv2d(\n",
      "      (_model): Sequential(\n",
      "        (0): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "        (1): Conv2d(4, 16, kernel_size=[8, 8], stride=(4, 4))\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): SlimConv2d(\n",
      "      (_model): Sequential(\n",
      "        (0): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
      "        (1): Conv2d(16, 32, kernel_size=[4, 4], stride=(2, 2))\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (2): SlimConv2d(\n",
      "      (_model): Sequential(\n",
      "        (0): Conv2d(32, 256, kernel_size=[11, 11], stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_value_branch): SlimFC(\n",
      "    (_model): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (advantage_module): Sequential(\n",
      "    (dueling_A_0): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (A): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (value_module): Sequential(\n",
      "    (dueling_V_0): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (V): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "0.75\n",
      "1.5\n",
      "1.4166666666666667\n",
      "1.0\n",
      "1.1304347826086956\n",
      "1.0714285714285714\n",
      "1.1612903225806452\n",
      "1.135135135135135\n",
      "1.1219512195121952\n",
      "1.0638297872340425\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents import dqn\n",
    "\n",
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "config['framework'] = 'torch'\n",
    "trainer = dqn.DQNTrainer(env='Breakout-v0', config=config)\n",
    "policy = trainer.get_policy()\n",
    "model = policy.q_model\n",
    "print(model)\n",
    "for i in range(10):\n",
    "    print(trainer.train()['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class FullyConnectedNetwork(TorchModelV2, nn.Module):\n",
    "    \"\"\"Generic fully connected network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        input_size = int(np.product(obs_space.shape))\n",
    "        self._hidden_layers = nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 256), \n",
    "            torch.nn.ReLU(), \n",
    "            torch.nn.Linear(256, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, num_outputs))\n",
    "        self._features = None\n",
    "\n",
    "        \n",
    "\n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict[\"obs_flat\"].float()\n",
    "        self._features = self._hidden_layers(obs)\n",
    "        return self._features, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-11 16:50:10,303\tINFO resource_spec.py:231 -- Starting Ray with 3.56 GiB memory available for workers and up to 1.8 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-10-11 16:50:10,774\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n",
      "2020-10-11 16:50:11,555\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import dqn\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "ModelCatalog.register_custom_model(\"my_model\", FullyConnectedNetwork)\n",
    "\n",
    "ray.init()\n",
    "trainer = dqn.DQNTrainer(env=\"Breakout-v0\", config={\n",
    "    \"framework\": \"torch\",\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"my_model\",\n",
    "        # Extra kwargs to be passed to your model's c'tor.\n",
    "        \"custom_model_config\": {},\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnectedNetwork_as_DQNTorchModel(\n",
      "  (_hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=28224, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (advantage_module): Sequential(\n",
      "    (dueling_A_0): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (A): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (value_module): Sequential(\n",
      "    (dueling_V_0): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (V): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "1.6666666666666667\n",
      "1.375\n",
      "1.5833333333333333\n",
      "1.4705882352941178\n",
      "1.5238095238095237\n",
      "1.48\n",
      "1.5172413793103448\n",
      "1.606060606060606\n",
      "1.5526315789473684\n",
      "1.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "policy = trainer.get_policy()\n",
    "model = policy.q_model\n",
    "print(model)\n",
    "for i in range(10):\n",
    "    print(trainer.train()['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
